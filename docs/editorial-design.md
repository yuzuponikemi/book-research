# 編集設計の原理

Project Cogito がなぜこの構造を持つのか — 技術的判断の背後にある思想的・編集的な設計原理を解説する。

---

## 三層アーキテクチャの設計思想

本システムは、哲学テキストを「ポッドキャスト」という新しいメディアに変換する。しかし、単なるフォーマット変換ではない。哲学的テキストの知的構造を保全しながら、対話という形式を通じて「哲学すること」そのものを再現する試みである。

この目標を達成するために、パイプラインは3つの独立した編集的役割に分離されている:

```
  Reader        Director       Dramaturg
  読み手         演出家          脚本家
  ─────────     ──────────     ──────────
  テキスト分析    構成設計         台本生成
  概念抽出       エピソード設計    対話執筆
  関係マッピング  認知的橋渡し     感情の起伏
```

この分離は、出版業界における編集プロセスに着想を得ている。一人の編集者がテキストを読み、構成を考え、文章を書くのではなく、それぞれの専門性を持つ担当者が段階的に関与する。各層は前の層の出力のみに依存し、独立してテストや改善ができる。

---

## 各レイヤーの編集的役割

### Reader（読み手）: 還元せず抽出する

Reader 層の使命は、テキストの知的構造を**還元することなく**抽出することにある。

一般的な要約（「デカルトは方法的懐疑を提唱した」）は、哲学的内容を命題に圧縮してしまう。しかし哲学テキストの価値は、結論だけでなく**そこに至る推論過程**、**解消されない緊張（アポリア）**、**修辞的な説得の技法**にある。

Reader 層は7つの次元を抽出する:

| # | 次元 | 従来の要約との違い |
|---|------|------------------|
| 1 | **概念（Concepts）** | 名前と説明だけでなく、原著からの引用を付して文脈を保持 |
| 2 | **アポリア（Aporias）** | 通常の要約では省略される「著者自身が解決できなかった問い」を明示的に抽出 |
| 3 | **関係（Relations）** | 概念間の依存・矛盾・発展を `depends_on` / `contradicts` / `evolves_into` で構造化 |
| 4 | **論理の流れ（Logic Flow）** | 結論ではなく、各議論の動機と推論の連鎖を追跡 |
| 5 | **論証構造（Arguments）** | 前提→結論の形式的構造を、論証タイプ（演繹/帰納/類推）とともに記録 |
| 6 | **名前付き哲学的手法（Named Moves）** | 方法的懐疑、コギト等の既知の技法を識別 |
| 7 | **修辞的戦略（Rhetorical Strategies）** | 隠喩・類推・思考実験など、読者を説得するための技法を記録 |

この7次元抽出により、「哲学テキストを読む」という行為の複雑さが保全される。下流のステージ（Director、Dramaturg）はこの豊かなデータを選択的に活用できる。

### Researcher / Critic（研究者・批評家）: テキストを歴史的対話の中に位置づける

哲学テキストは真空の中に存在しない。すべての哲学的主張は、先行する議論への応答であり、後続の批判の対象となる。

Researcher 層は Web 検索と参考文献から**外部的文脈**を収集し、Critic 層はそれを活用して各概念に対する**歴史的批判と反論**を生成する。

この二段構成の意図は:

- **Reader の分析は純粋に保つ**: テキスト内部の構造分析に外部情報を混入させない
- **研究と批評は後から注入する**: Plan（Stage 4）と Script（Stage 5）にのみ反映させ、分析の客観性を損なわない
- **`--skip-research` で無効化できる**: テキスト自体の分析だけで完結するミニマルなパイプラインも可能

### Director（演出家）: エピソード構成を設計する

Director 層は、概念グラフを**聴取体験**に変換する。哲学的に正確であることと、リスナーが10分間聴き続けたいと思うことは、異なる要件である。

Director は3つのモードを通じて異なる編集判断を下す:

| モード | 編集的判断 |
|--------|-----------|
| **essence** | 著作全体を1つのアポリアに凝縮する — 「この本を一言で言えば何が問題なのか」 |
| **curriculum** | 著作の論理的進行に忠実な6回シリーズを設計する — 教育的な段階性を重視 |
| **topic** | 特定のトピック（例: 心身二元論）を深掘りする — 横断的な概念の切り出し |

各エピソードには**認知的ブリッジ（cognitive_bridge）**が含まれる。これは17世紀の哲学を21世紀のリスナーに接続するための「橋」であり、AI、SNS、スタートアップ、生命倫理といった現代的な文脈を通じて、古典の問いを「自分ごと」にする装置である。

### Dramaturg（脚本家）: 対話として哲学を具現化する

Dramaturg 層は、パイプラインの中で唯一「哲学をする」ステージである。

ここでの設計上の核心は、対話が単なる「知識の伝達」ではなく、**認識のプロセスそのもの**であるという点にある。2人の登場人物が互いに問い、反論し、考えを深める過程そのものが、聴取者にとっての哲学的体験となる。

台本は3幕構成を採用する:

- **第1幕（導入）**: 現代の具体的な場面から哲学的問いを引き出す — リスナーの「フック」
- **第2幕（掘り下げ）**: 原著の概念を対話の中で展開し、引用を織り込み、対立を生む
- **第3幕（余韻）**: 完全な答えを出さずに問いを残す — 思考の続きはリスナーに委ねる

この構成は、ソクラテス的対話法（結論ではなく問いを通じて思考を深める）に直接的に影響を受けている。

---

## 5つの設計原則

### 1. 還元より保全（Preservation over Reduction）

従来のテキスト要約は、「この本の要点は3つ」のように情報を圧縮する。しかし哲学テキストにおいて、要点に還元できない部分 — アポリア、修辞的戦略、論証の微細な構造 — こそが本質的な価値を持つ。

本システムは7次元の抽出と、10〜20概念・4〜8アポリアという豊かな概念グラフによって、テキストの複雑さを可能な限り保全する。下流のステージは必要に応じて選択的に利用するが、データそのものは削減しない。

### 2. 歴史性（Historicity）

哲学的主張は歴史的真空には存在しない。デカルトのコギトはアルノーの循環論法批判なしには理解できず、方法的懐疑はモンテーニュの懐疑主義を踏まえなければ文脈を失う。

書籍設定ファイルに `context.notable_critics` を設け、Critic 層が各概念に対する歴史的批判を生成する仕組みは、テキストを**歴史的対話のネットワーク**の中に位置づけるためのものである。

### 3. アポリアの誠実さ（Aporetic Honesty）

アポリア（解決不能な哲学的緊張）は、要約やポッドキャストにおいて真っ先に省略される要素である。「デカルトは〜を主張した」とは言えても、「デカルトは〜を解決できなかった」とは言いにくい。

本システムはアポリアをパイプラインの中核データとして扱う。概念グラフにアポリアを明示し、エピソード設計にアポリアIDを組み込み、台本プロンプトに「簡単には答えを出さないこと」と明記する。これにより、知的誠実さと聴取体験の面白さの両立を図る。

### 4. 対話としての認識論（Dialogue as Epistemology）

ポッドキャストは音声メディアであり、対話は単なる表現形式ではない。2人の異なる視点を持つ話者が問い合うことで、モノローグでは到達できない認識の深さに到達しうる。

ペルソナシステムは3つのプリセットを提供するが、これらは単なる「キャラクター設定」ではなく、**認識論的な立場**の表現である:

- `descartes_default` — 現代の懐疑論者 vs. デカルト本人の亡霊（時代を超えた直接対話）
- `socratic` — 哲学初心者 vs. 案内人（問いを通じた発見的学習）
- `debate` — 擁護者 vs. 批判者（弁証法的対立による真理の追求）

### 5. 多言語の橋渡し（Multilingual Bridge）

哲学テキストの多くは西欧語で書かれている。日本語話者がこれらにアクセスするためには翻訳が必要だが、単なる逐語訳ではなく、概念を日本語の文化的文脈で再解釈する必要がある。

本システムは以下の多言語戦略を採用する:

- **Reader 層（分析）は英語で実行**: LLM の分析精度を最大化するため
- **Dramaturg 層（台本）は日本語で出力**: プロンプト自体が日本語であり、自然な対話を生成
- **Enricher 層（統合）は英日両方を生成**: `enrichment_summary`（EN）と `enrichment_summary_ja`（JA）
- **Translator（翻訳）で中間出力を日本語化**: 分析レポート等の英語出力も日本語版を提供

---

## ペルソナシステムの設計意図

ペルソナは `config/personas.yaml` で定義される。各プリセットは2人の登場人物（`persona_a`, `persona_b`）と、VOICEVOX 音声 ID のマッピング（`voice`）で構成される。

```yaml
presets:
  descartes_default:
    persona_a:
      name: "Host"
      role: "現代の懐疑論者"
      tone: "冷静・知的・好奇心旺盛"
      speaking_style: "Uses modern analogies (coding, startups, AI)"
    persona_b:
      name: "Descartes"
      role: "情熱的な哲学者の亡霊"
      tone: "情熱的・論理的・時に皮肉"
      speaking_style: "Mixes 17th-century rigor with modern insight"
    voice:
      Host: 3          # 雨晴はう（冷静・知的）
      Descartes: 0     # 四国めたん（表現力豊か）
```

ペルソナの各フィールドがどのように使われるか:

| フィールド | 使用箇所 | 効果 |
|-----------|---------|------|
| `name` | 台本の話者名 | 対話の構造を規定 |
| `role` | 台本プロンプト | LLM にキャラクターの立場を伝える |
| `description` | 台本プロンプト | 思考の深さと個性を規定 |
| `tone` | 台本プロンプト | 発話のトーンを制御（冷静/情熱的/皮肉 等） |
| `speaking_style` | 台本プロンプト | 具体的な話し方の特徴を指示 |
| `voice` | 音声合成（Stage 6） | 登場人物を異なる VOICEVOX 音声に割り当て |

音声 ID は登場人物名で直接マッピングされ、マッチしない場合は `_default_a` / `_default_b` にフォールバックする。

---

## 技術スタック

| 技術 | 役割 | 備考 |
|------|------|------|
| **Ollama** | ローカル LLM 推論 | 全 LLM コールの基盤。GPU メモリ管理が重要 |
| **LangChain** | LLM 呼び出しの抽象化 | `ChatOllama` ラッパーを利用 |
| **Pydantic** | データモデル / バリデーション | `ConceptGraph`, `Syllabus`, `Script` 等の構造を保証 |
| **PyYAML** | 設定ファイル読み込み | 書籍設定、ペルソナ設定 |
| **httpx** | HTTP クライアント | テキストダウンロード、VOICEVOX API 通信 |
| **Tavily** | Web 検索 API（有料） | 高品質な検索結果。`TAVILY_API_KEY` が必要 |
| **DuckDuckGo (`ddgs`)** | Web 検索（無料） | Tavily のフォールバック。API キー不要 |
| **VOICEVOX** | 日本語音声合成エンジン | ローカル実行。無料。多数のキャラクター音声 |
| **pydub** | オーディオ処理 | WAV→MP3 変換、無音挿入、音声結合 |
| **ffmpeg** | メディアエンコーディング | pydub の MP3 エクスポートに必要 |
| **TranslateGemma** | 英日翻訳モデル | Ollama 上で動作する翻訳特化モデル |

---

## LLM の使い分け

本システムでは、タスクの性質に応じて LLM のパラメータを使い分ける。

### Reader / Director 層（分析・構成）

```python
ChatOllama(model="llama3", temperature=0.1, num_ctx=16384, format="json")
```

- **低温度（0.1）**: 分析の再現性と正確性を重視
- **format="json"**: JSON 出力を強制し、パース失敗を最小化
- **大きなコンテキスト長**: テキストチャンク全体を一度に処理

分析的タスクでは、創造性よりも正確性が重要である。同じテキストを同じモデルで分析すれば、ほぼ同じ概念グラフが得られることが望ましい。

### Dramaturg 層（台本生成）

```python
ChatOllama(model="qwen3-next", temperature=0.7, num_ctx=32768)
```

- **高温度（0.7）**: 対話の自然さと創造性を重視
- **format 指定なし**: 自由な日本語テキスト生成を許可（JSON は `extract_json()` で後から抽出）
- **大容量モデル**: 自然な日本語対話の生成には、大きなモデルが必要

台本生成では、毎回異なる対話が生まれることがむしろ望ましい。同じ概念を扱っても、話の展開や比喩の選択は実行ごとに異なりうる。

### Enricher 層（統合）

```python
ChatOllama(model="llama3", temperature=0.2, num_ctx=32768, num_predict=8192, format="json")
```

- **やや高い温度（0.2）**: 要約の流暢さを確保しつつ、事実の正確性を維持
- **`num_predict=8192`**: 長い日本語要約の生成に十分なトークン数を確保

### Translator 層（翻訳）

```python
ChatOllama(model="translategemma:12b", temperature=0.1, num_ctx=8192)
```

- **翻訳特化モデル**: 汎用 LLM ではなく翻訳に最適化されたモデルを使用
- **低温度**: 翻訳の忠実性を最大化
- **小さなコンテキスト**: セクション単位の分割翻訳で対応
